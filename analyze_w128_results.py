"""
Analyze W=128 curriculum training results and compare with W=64 baseline.
"""

import json
import glob
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path

# Load W=128 results
w128_dir = Path("curriculum_exp_w128")
w128_checkpoints = sorted(glob.glob(str(w128_dir / "checkpoint_*/summary.json")))

w128_steps = []
w128_ttt_1xW = []
w128_sw_1xW = []
w128_ttt_10xW = []
w128_sw_10xW = []

for ckpt_path in w128_checkpoints:
    with open(ckpt_path) as f:
        data = json.load(f)
    w128_steps.append(data["step"])
    w128_ttt_1xW.append(data["ttt_ppl_at_1xW"])
    w128_sw_1xW.append(data["sw_ppl_at_1xW"])
    w128_ttt_10xW.append(data["ttt_ppl_at_10xW"])
    w128_sw_10xW.append(data["sw_ppl_at_10xW"])

# Load W=64 results for comparison
w64_dir = Path("curriculum_exp")
w64_checkpoints = sorted(glob.glob(str(w64_dir / "checkpoint_*/summary.json")))

w64_steps = []
w64_ttt_1xW = []
w64_sw_1xW = []
w64_ttt_10xW = []
w64_sw_10xW = []

for ckpt_path in w64_checkpoints:
    with open(ckpt_path) as f:
        data = json.load(f)
    w64_steps.append(data["step"])
    w64_ttt_1xW.append(data["ttt_ppl_at_1xW"])
    w64_sw_1xW.append(data["sw_ppl_at_1xW"])
    w64_ttt_10xW.append(data["ttt_ppl_at_10xW"])
    w64_sw_10xW.append(data["sw_ppl_at_10xW"])

# Create comprehensive comparison plot
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# Plot 1: TTT at 1√óW (convergence)
ax = axes[0, 0]
ax.plot(w128_steps, w128_ttt_1xW, 'b-', linewidth=2, label='W=128 TTT@1√óW', alpha=0.8)
ax.plot(w128_steps, w128_sw_1xW, 'b--', linewidth=2, label='W=128 SW@1√óW', alpha=0.6)
ax.plot(w64_steps, w64_ttt_1xW, 'r-', linewidth=2, label='W=64 TTT@1√óW', alpha=0.8)
ax.plot(w64_steps, w64_sw_1xW, 'r--', linewidth=2, label='W=64 SW@1√óW', alpha=0.6)
ax.set_xlabel('Training Steps', fontsize=12)
ax.set_ylabel('Perplexity', fontsize=12)
ax.set_title('Training Window Perplexity (1√óW) Convergence', fontsize=14, fontweight='bold')
ax.legend(fontsize=10)
ax.grid(True, alpha=0.3)
ax.set_yscale('log')

# Find best checkpoint
best_w128_idx = np.argmin(w128_ttt_1xW)
best_w128_step = w128_steps[best_w128_idx]
best_w128_ppl = w128_ttt_1xW[best_w128_idx]
ax.axvline(best_w128_step, color='blue', linestyle=':', alpha=0.5)
ax.text(best_w128_step, best_w128_ppl * 1.1, f'Best W=128\nStep {best_w128_step}\nPPL {best_w128_ppl:.1f}', 
        ha='center', fontsize=9, bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))

best_w64_idx = np.argmin(w64_ttt_1xW)
best_w64_step = w64_steps[best_w64_idx]
best_w64_ppl = w64_ttt_1xW[best_w64_idx]
ax.axvline(best_w64_step, color='red', linestyle=':', alpha=0.5)
ax.text(best_w64_step, best_w64_ppl * 1.1, f'Best W=64\nStep {best_w64_step}\nPPL {best_w64_ppl:.1f}', 
        ha='center', fontsize=9, bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.5))

# Plot 2: Length generalization (10√óW)
ax = axes[0, 1]
ax.plot(w128_steps, w128_ttt_10xW, 'b-', linewidth=2, label='W=128 TTT@10√óW', alpha=0.8)
ax.plot(w128_steps, w128_sw_10xW, 'b--', linewidth=2, label='W=128 SW@10√óW', alpha=0.6)
ax.plot(w64_steps, w64_ttt_10xW, 'r-', linewidth=2, label='W=64 TTT@10√óW', alpha=0.8)
ax.plot(w64_steps, w64_sw_10xW, 'r--', linewidth=2, label='W=64 SW@10√óW', alpha=0.6)
ax.set_xlabel('Training Steps', fontsize=12)
ax.set_ylabel('Perplexity', fontsize=12)
ax.set_title('Long Sequence Perplexity (10√óW) Evolution', fontsize=14, fontweight='bold')
ax.legend(fontsize=10)
ax.grid(True, alpha=0.3)
ax.set_yscale('log')

# Annotate crossover points
w128_crossover_idx = None
for i in range(len(w128_steps)):
    if w128_ttt_10xW[i] < w128_sw_10xW[i]:
        w128_crossover_idx = i
        break

if w128_crossover_idx:
    ax.axvline(w128_steps[w128_crossover_idx], color='green', linestyle=':', alpha=0.5)
    ax.text(w128_steps[w128_crossover_idx], min(w128_ttt_10xW), 
            f'TTT surpasses SW\nStep {w128_steps[w128_crossover_idx]}', 
            rotation=90, va='bottom', fontsize=9, 
            bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))

# Plot 3: SW/TTT ratio at 10√óW
ax = axes[1, 0]
w128_ratio = np.array(w128_sw_10xW) / np.array(w128_ttt_10xW)
w64_ratio = np.array(w64_sw_10xW) / np.array(w64_ttt_10xW)
ax.plot(w128_steps, w128_ratio, 'b-', linewidth=2, label='W=128 SW/TTT@10√óW', alpha=0.8)
ax.plot(w64_steps, w64_ratio, 'r-', linewidth=2, label='W=64 SW/TTT@10√óW', alpha=0.8)
ax.axhline(1.0, color='black', linestyle='--', alpha=0.5, label='Equal Performance')
ax.fill_between(ax.get_xlim(), 0.95, 1.05, color='gray', alpha=0.2, label='¬±5% band')
ax.set_xlabel('Training Steps', fontsize=12)
ax.set_ylabel('SW/TTT Ratio', fontsize=12)
ax.set_title('Length Generalization Ratio (SW/TTT at 10√óW)', fontsize=14, fontweight='bold')
ax.legend(fontsize=10)
ax.grid(True, alpha=0.3)
ax.set_ylim(0.7, 1.3)

# Add text annotations for final ratios
final_w128_ratio = w128_ratio[-1]
final_w64_ratio = w64_ratio[-1]
ax.text(w128_steps[-1], final_w128_ratio, f'{final_w128_ratio:.3f}', 
        ha='left', va='center', fontsize=10, color='blue', fontweight='bold')
ax.text(w64_steps[-1], final_w64_ratio, f'{final_w64_ratio:.3f}', 
        ha='left', va='center', fontsize=10, color='red', fontweight='bold')

# Plot 4: Summary statistics table
ax = axes[1, 1]
ax.axis('off')

# Prepare summary data
w128_final = {
    'step': w128_steps[-1],
    'ttt_1xW': w128_ttt_1xW[-1],
    'sw_1xW': w128_sw_1xW[-1],
    'ttt_10xW': w128_ttt_10xW[-1],
    'sw_10xW': w128_sw_10xW[-1],
}

w128_best = {
    'step': best_w128_step,
    'ttt_1xW': best_w128_ppl,
    'sw_1xW': w128_sw_1xW[best_w128_idx],
    'ttt_10xW': w128_ttt_10xW[best_w128_idx],
    'sw_10xW': w128_sw_10xW[best_w128_idx],
}

w64_final = {
    'step': w64_steps[-1],
    'ttt_1xW': w64_ttt_1xW[-1],
    'sw_1xW': w64_sw_1xW[-1],
    'ttt_10xW': w64_ttt_10xW[-1],
    'sw_10xW': w64_sw_10xW[-1],
}

w64_best = {
    'step': best_w64_step,
    'ttt_1xW': best_w64_ppl,
    'sw_1xW': w64_sw_1xW[best_w64_idx],
    'ttt_10xW': w64_ttt_10xW[best_w64_idx],
    'sw_10xW': w64_sw_10xW[best_w64_idx],
}

# Create table
table_data = [
    ['Metric', 'W=128 Best', 'W=128 Final', 'W=64 Best', 'W=64 Final'],
    ['Training Steps', f"{w128_best['step']:,}", f"{w128_final['step']:,}", f"{w64_best['step']:,}", f"{w64_final['step']:,}"],
    ['', '', '', '', ''],
    ['TTT PPL@1√óW', f"{w128_best['ttt_1xW']:.1f}", f"{w128_final['ttt_1xW']:.1f}", f"{w64_best['ttt_1xW']:.1f}", f"{w64_final['ttt_1xW']:.1f}"],
    ['SW PPL@1√óW', f"{w128_best['sw_1xW']:.1f}", f"{w128_final['sw_1xW']:.1f}", f"{w64_best['sw_1xW']:.1f}", f"{w64_final['sw_1xW']:.1f}"],
    ['SW/TTT@1√óW', f"{w128_best['sw_1xW']/w128_best['ttt_1xW']:.3f}", f"{w128_final['sw_1xW']/w128_final['ttt_1xW']:.3f}", 
     f"{w64_best['sw_1xW']/w64_best['ttt_1xW']:.3f}", f"{w64_final['sw_1xW']/w64_final['ttt_1xW']:.3f}"],
    ['', '', '', '', ''],
    ['TTT PPL@10√óW', f"{w128_best['ttt_10xW']:.1f}", f"{w128_final['ttt_10xW']:.1f}", f"{w64_best['ttt_10xW']:.1f}", f"{w64_final['ttt_10xW']:.1f}"],
    ['SW PPL@10√óW', f"{w128_best['sw_10xW']:.1f}", f"{w128_final['sw_10xW']:.1f}", f"{w64_best['sw_10xW']:.1f}", f"{w64_final['sw_10xW']:.1f}"],
    ['SW/TTT@10√óW', f"{w128_best['sw_10xW']/w128_best['ttt_10xW']:.3f}", f"{w128_final['sw_10xW']/w128_final['ttt_10xW']:.3f}", 
     f"{w64_best['sw_10xW']/w64_best['ttt_10xW']:.3f}", f"{w64_final['sw_10xW']/w64_final['ttt_10xW']:.3f}"],
]

# Color cells based on performance
cell_colors = []
for i, row in enumerate(table_data):
    if i == 0:  # Header row
        cell_colors.append(['lightgray'] * 5)
    elif i in [2, 6]:  # Empty rows
        cell_colors.append(['white'] * 5)
    else:
        row_colors = ['white'] * 5  # Default white
        
        # Highlight best values in each row (excluding metric name column)
        if i >= 3:  # Data rows
            try:
                values = [float(row[j].replace(',', '')) for j in range(1, 5)]
                if 'SW/TTT' in row[0]:
                    # For ratios, lower is better (except when highlighting SW advantage)
                    best_idx = np.argmin(values) + 1
                else:
                    # For PPL, lower is better
                    best_idx = np.argmin(values) + 1
                row_colors[best_idx] = 'lightgreen'
            except:
                pass
        
        cell_colors.append(row_colors)

table = ax.table(cellText=table_data, cellLoc='center', loc='center',
                cellColours=cell_colors, bbox=[0, 0, 1, 1])
table.auto_set_font_size(False)
table.set_fontsize(9)
table.scale(1, 2)

# Style header row
for i in range(5):
    table[(0, i)].set_facecolor('darkgray')
    table[(0, i)].set_text_props(weight='bold', color='white')

# Bold metric names
for i in range(1, len(table_data)):
    table[(i, 0)].set_text_props(weight='bold', ha='left')

ax.set_title('Performance Summary Comparison', fontsize=14, fontweight='bold', pad=20)

plt.tight_layout()
plt.savefig('W128_vs_W64_comprehensive_analysis.png', dpi=150, bbox_inches='tight')
print("‚úÖ Comprehensive analysis plot saved to: W128_vs_W64_comprehensive_analysis.png")

# Print detailed summary
print("\n" + "="*80)
print("W=128 CURRICULUM TRAINING ANALYSIS")
print("="*80)

print(f"\nüéØ Training Summary:")
print(f"  Total training steps: {w128_steps[-1]:,} (stopped at ~45% of 100k due to early stopping)")
print(f"  Total training time: 225 minutes (3.75 hours)")
print(f"  Training speed: ~200 steps/minute (~4 it/s)")
print(f"  Checkpoints evaluated: {len(w128_checkpoints)}")
print(f"  Early stopping: Triggered after 10 checkpoints without improvement")

print(f"\nüìä Best Checkpoint (Step {best_w128_step:,}):")
print(f"  TTT PPL@1√óW: {w128_best['ttt_1xW']:.1f}")
print(f"  SW PPL@1√óW:  {w128_best['sw_1xW']:.1f} (ratio: {w128_best['sw_1xW']/w128_best['ttt_1xW']:.3f})")
print(f"  TTT PPL@10√óW: {w128_best['ttt_10xW']:.1f}")
print(f"  SW PPL@10√óW:  {w128_best['sw_10xW']:.1f} (ratio: {w128_best['sw_10xW']/w128_best['ttt_10xW']:.3f})")

print(f"\nüìä Final Checkpoint (Step {w128_final['step']:,}):")
print(f"  TTT PPL@1√óW: {w128_final['ttt_1xW']:.1f}")
print(f"  SW PPL@1√óW:  {w128_final['sw_1xW']:.1f} (ratio: {w128_final['sw_1xW']/w128_final['ttt_1xW']:.3f})")
print(f"  TTT PPL@10√óW: {w128_final['ttt_10xW']:.1f}")
print(f"  SW PPL@10√óW:  {w128_final['sw_10xW']:.1f} (ratio: {w128_final['sw_10xW']/w128_final['ttt_10xW']:.3f})")

print(f"\nüîç Comparison with W=64 (Final):")
print(f"  W=64 trained for {w64_steps[-1]:,} steps")
print(f"  W=128 trained for {w128_steps[-1]:,} steps (2.96√ó longer)")
print(f"\n  PPL@1√óW improvement:")
print(f"    TTT: {w64_final['ttt_1xW']:.1f} ‚Üí {w128_final['ttt_1xW']:.1f} ({(w128_final['ttt_1xW']/w64_final['ttt_1xW']-1)*100:+.1f}%)")
print(f"    SW:  {w64_final['sw_1xW']:.1f} ‚Üí {w128_final['sw_1xW']:.1f} ({(w128_final['sw_1xW']/w64_final['sw_1xW']-1)*100:+.1f}%)")
print(f"\n  PPL@10√óW improvement:")
print(f"    TTT: {w64_final['ttt_10xW']:.1f} ‚Üí {w128_final['ttt_10xW']:.1f} ({(w128_final['ttt_10xW']/w64_final['ttt_10xW']-1)*100:+.1f}%)")
print(f"    SW:  {w64_final['sw_10xW']:.1f} ‚Üí {w128_final['sw_10xW']:.1f} ({(w128_final['sw_10xW']/w64_final['sw_10xW']-1)*100:+.1f}%)")

print(f"\nüéì Key Findings:")

# Finding 1: TTT convergence
w128_improvement = (w128_ttt_1xW[0] - w128_best['ttt_1xW']) / w128_ttt_1xW[0] * 100
print(f"  1. TTT convergence: Improved from {w128_ttt_1xW[0]:.0f} to {w128_best['ttt_1xW']:.1f} PPL ({w128_improvement:.1f}% reduction)")

# Finding 2: Length generalization evolution
if w128_crossover_idx:
    print(f"  2. Length generalization: TTT surpassed SW at step {w128_steps[w128_crossover_idx]:,}")
    print(f"     Final SW/TTT@10√óW ratio: {w128_final['sw_10xW']/w128_final['ttt_10xW']:.3f} (TTT outperforms by {(1-w128_final['sw_10xW']/w128_final['ttt_10xW'])*100:.1f}%)")
else:
    print(f"  2. Length generalization: SW maintained advantage throughout training")
    print(f"     Final SW/TTT@10√óW ratio: {w128_final['sw_10xW']/w128_final['ttt_10xW']:.3f}")

# Finding 3: Window size effect
print(f"  3. Window size effect:")
print(f"     W=64:  SW/TTT@10√óW = {w64_final['sw_10xW']/w64_final['ttt_10xW']:.3f} (SW advantage: {(1-w64_final['sw_10xW']/w64_final['ttt_10xW'])*100:.1f}%)")
print(f"     W=128: SW/TTT@10√óW = {w128_final['sw_10xW']/w128_final['ttt_10xW']:.3f} (TTT advantage: {(1-w128_final['sw_10xW']/w128_final['ttt_10xW'])*100:.1f}%)")
print(f"     ‚Üí Larger window enables TTT to leverage long-range context more effectively!")

# Finding 4: Training efficiency
steps_to_convergence_w64 = w64_steps[-1]
steps_to_convergence_w128 = best_w128_step
print(f"  4. Training efficiency:")
print(f"     W=64:  {steps_to_convergence_w64:,} steps to reach PPL {w64_best['ttt_1xW']:.1f}")
print(f"     W=128: {steps_to_convergence_w128:,} steps to reach PPL {w128_best['ttt_1xW']:.1f}")
print(f"     ‚Üí W=128 requires {steps_to_convergence_w128/steps_to_convergence_w64:.1f}√ó more steps for better convergence")

print("\n" + "="*80)

# Training Scripts Summary

## ğŸ“ Files Created

### Main Training Script
- **`train_ttt.py`** (11 KB)
  - Full-featured training script with all options
  - Supports all model sizes (125m, 350m, 760m, 1b)
  - Multi-GPU with Accelerate
  - W&B integration
  - Checkpointing and resumption

### Model-Specific Launch Scripts
- **`train_125m_c4.sh`** - Launch TTT-125M training
- **`train_350m_c4.sh`** - Launch TTT-350M training
- **`train_760m_c4.sh`** - Launch TTT-760M training
- **`train_1b_c4.sh`** - Launch TTT-1B training

### Interactive Launcher
- **`launch_training.sh`** - Menu-based training launcher

### Documentation
- **`QUICKSTART.md`** - Quick start guide
- **`TRAINING_RECIPES.md`** - Detailed training documentation
- **`DATASET_CHANGE.md`** - Dataset migration guide (WikiText-2 â†’ C4)

## ğŸ¯ Training Recipes (from JAX codebase)

All recipes follow the official configurations:

```
Common Settings:
â”œâ”€â”€ Dataset: C4 English
â”œâ”€â”€ Sequence Length: 2048 tokens
â”œâ”€â”€ Global Batch Size: 256 sequences (0.5M tokens/step)
â”œâ”€â”€ Weight Decay: 0.1
â”œâ”€â”€ Optimizer: AdamW
â”œâ”€â”€ Mixed Precision: BF16
â””â”€â”€ TTT Layer: Linear (ttt_base_lr=1.0)

TTT-125M:
â”œâ”€â”€ Steps: 4,800
â”œâ”€â”€ Learning Rate: 3e-3 â†’ 1e-5
â”œâ”€â”€ Warmup: 480 steps (10%)
â””â”€â”€ Total Tokens: ~2.4B

TTT-350M:
â”œâ”€â”€ Steps: 13,500
â”œâ”€â”€ Learning Rate: 1.5e-3 â†’ 1e-5
â”œâ”€â”€ Warmup: 1,350 steps (10%)
â””â”€â”€ Total Tokens: ~6.75B

TTT-760M:
â”œâ”€â”€ Steps: 29,000
â”œâ”€â”€ Learning Rate: 1.25e-3 â†’ 1e-5
â”œâ”€â”€ Warmup: 2,900 steps (10%)
â””â”€â”€ Total Tokens: ~14.5B

TTT-1B:
â”œâ”€â”€ Steps: 50,000
â”œâ”€â”€ Learning Rate: 1e-3 â†’ 1e-5
â”œâ”€â”€ Warmup: 5,000 steps (10%)
â””â”€â”€ Total Tokens: ~25B
```

## ğŸš€ Usage Examples

### Quick Start
```bash
# Interactive launcher
./launch_training.sh

# Direct launch
./train_125m_c4.sh
```

### Custom Training
```bash
# Train with custom parameters
python train_ttt.py \
    --model_size "125m" \
    --dataset_name "allenai/c4" \
    --dataset_config "en" \
    --seq_length 2048 \
    --per_device_train_batch_size 32 \
    --max_train_steps 4800 \
    --learning_rate 3e-3 \
    --output_dir "./my_experiment"
```

### Resume Training
```bash
python train_ttt.py \
    --model_size "125m" \
    --resume_from_checkpoint "experiments/checkpoint-3000" \
    --max_train_steps 10000
```

## ğŸ”§ Key Parameters

### Model Configuration
- `--model_size`: "125m", "350m", "760m", "1b"
- `--ttt_layer_type`: "linear", "mlp"
- `--ttt_base_lr`: TTT-specific learning rate (default: 1.0)

### Dataset Configuration
- `--dataset_name`: "allenai/c4" (default)
- `--dataset_config`: "en" (English), "de", "es", etc.
- `--seq_length`: Sequence length in tokens (default: 2048)

### Training Configuration
- `--per_device_train_batch_size`: Batch size per GPU
- `--gradient_accumulation_steps`: Gradient accumulation
- `--max_train_steps`: Total training steps
- `--learning_rate`: Peak learning rate
- `--lr_end`: Final learning rate (default: 1e-5)
- `--lr_warmup_steps`: Warmup steps
- `--weight_decay`: AdamW weight decay (default: 0.1)
- `--mixed_precision`: "no", "fp16", "bf16" (default)

### Checkpointing
- `--save_checkpoint_freq`: Save every N steps (default: 1000)
- `--save_milestone_freq`: Save milestone every N steps (default: 2000)
- `--output_dir`: Output directory

### Logging
- `--logging_steps`: Log every N steps (default: 10)
- `--wandb_project`: W&B project name
- `--wandb_run_name`: W&B run name

## ğŸ“Š Expected Outputs

### During Training
```
***** Running training *****
  Max steps = 4800
  Per device batch size = 32
  Gradient accumulation steps = 1
  Total batch size = 256

Step 10: loss=8.2345, lr=6.25e-04
Step 20: loss=7.8912, lr=1.25e-03
...
Checkpoint saved to experiments/ttt_125m_c4_xxx/checkpoint-1000
```

### Directory Structure
```
experiments/ttt_125m_c4_20251016_123456/
â”œâ”€â”€ checkpoint-1000/
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚   â””â”€â”€ training_state.pt
â”œâ”€â”€ milestone-2000/
â”œâ”€â”€ checkpoint-3000/
â”œâ”€â”€ final_model/
â””â”€â”€ training_args.json
```

## ğŸ“ Best Practices

1. **Start Small**: Test with 125M before scaling up
2. **Monitor Closely**: Use W&B for real-time tracking
3. **Save Often**: Checkpoints every 1K steps (already configured)
4. **Test Recovery**: Verify checkpoint resumption works
5. **Check GPU Usage**: Ensure all GPUs are utilized (`nvidia-smi`)

## ğŸ” Verification

Before starting a long training run:

```bash
# Test training for 10 steps
python train_ttt.py \
    --model_size "125m" \
    --max_train_steps 10 \
    --output_dir "./test_run"

# Verify checkpoint loading
python train_ttt.py \
    --model_size "125m" \
    --resume_from_checkpoint "./test_run/checkpoint-10" \
    --max_train_steps 20 \
    --output_dir "./test_run"
```

## ğŸ“ˆ Performance Tuning

### Memory Optimization
- Reduce `per_device_train_batch_size`
- Increase `gradient_accumulation_steps`
- Use gradient checkpointing (requires code modification)

### Speed Optimization
- Use BF16 (enabled by default)
- Ensure fast storage for dataset cache
- Use multiple dataloading workers (requires code modification)

## ğŸ†˜ Troubleshooting

| Issue | Solution |
|-------|----------|
| OOM Error | Reduce batch size, increase grad accumulation |
| Slow training | Check GPU utilization, verify BF16 enabled |
| Dataset download fails | Check internet, set HF_DATASETS_CACHE |
| Checkpoint not found | Verify path, check permissions |
| W&B not logging | Run `wandb login`, check project name |

## ğŸ“š Related Documentation

- **README.md** - General project information
- **QUICKSTART.md** - Quick start guide
- **TRAINING_RECIPES.md** - Detailed training guide
- **DATASET_CHANGE.md** - Dataset information

## ğŸ¯ Next Steps

1. âœ… Review `QUICKSTART.md` for quick start
2. âœ… Read `TRAINING_RECIPES.md` for details
3. âœ… Run `./launch_training.sh` to start training
4. âœ… Monitor training on W&B dashboard
5. âœ… Evaluate trained models on your tasks

---

**Happy Training!** ğŸš€
